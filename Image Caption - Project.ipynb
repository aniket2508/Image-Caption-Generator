{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.applications import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import keras.preprocessing.image as pic\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import pickle as pk    #for saving features\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Dense, Embedding, Add, LSTM, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "basepath = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GETTING AND PREPROCESSING RAW DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting features from each image into a 4096 dimensional vector using VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_images():\n",
    "    \n",
    "    model = VGG16()\n",
    "    model.layers.pop()  # The last softmax later is not required, we only need to 4096 dimensional encoding\n",
    "    model = Model(inputs = model.inputs, outputs = model.layers[-1].output)\n",
    "    \n",
    "    img_set = 'Flicker8k_Dataset'\n",
    "    img_dir = os.path.join(basepath, img_set)\n",
    "    features = {}\n",
    "    \n",
    "    for img_id in os.listdir(img_dir):\n",
    "        \n",
    "        path = os.path.join(basepath, img_id)\n",
    "        image = pic.load_img(path, target_size = (224, 224))   #input image in VGG model is (224, 224)\n",
    "        image = pic.img_to_array(image) #converting image to pixels\n",
    "        image = image.reshape(1, image.shape[0], image.shape[1], -1) #converting to (1, nh, nw, nc)\n",
    "        image = preprocess_input(image)  #The images are converted from RGB to BGR, then each color channel is zero-centered with respect to the ImageNet dataset, without scaling.\n",
    "        feature_image = model.predict(image)\n",
    "        img_name = img_id.split('.')[0]  #each image in directory is \"img_id.jpg\"\n",
    "        features[img_name] = feature_image\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'Flicker8k_Text\\Flickr8k.token.txt'\n",
    "b = os.path.join(basepath, a)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting captions associated with each image id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_captions():\n",
    "    \n",
    "    text = []\n",
    "    captions = {}\n",
    "    \n",
    "    caption_file = 'Flicker8k_Text\\Flickr8k.token.txt'\n",
    "    with open(os.path.join(basepath, caption_file), 'r') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    for text in text.split('\\n'):\n",
    "        \n",
    "        words = text.split()\n",
    "        if(len(words) < 2):  #if there is no caption provided\n",
    "            continue\n",
    "            \n",
    "        img_id, caption = words[0], words[1:]\n",
    "        img_id = img_id.split('.')[0]\n",
    "        caption = ' '.join(caption)\n",
    "        \n",
    "        if(img_id not in captions.keys()):  #as each image id has multiple captions, we make a list of all of them\n",
    "            captions[img_id] = []\n",
    "            \n",
    "        captions[img_id].append(caption)\n",
    "    \n",
    "    return captions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(captions):\n",
    "    \n",
    "    punc = string.punctuation\n",
    "    \n",
    "    for image in captions.keys():\n",
    "        for i, caption in enumerate(captions[image]):\n",
    "            new_caption = []\n",
    "            for word in caption.split():\n",
    "                \n",
    "                if(len(word) < 2):   #ignoring words like 'a'\n",
    "                    continue\n",
    "                word = word.lower()\n",
    "                t = [w for w in word if w not in punc] #removing punctuations\n",
    "                word = \"\".join(t)\n",
    "                word = re.sub(r\"\\d+\", \"\", word) #removing numbers\n",
    "                new_caption.append(word)\n",
    "                \n",
    "            new_caption = 'startseq ' + ' '.join(new_caption) + ' endseq'\n",
    "            captions[image][i] = new_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = encode_images()    #takes about 1 hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' #to save in file:\n",
    "f = open('feature_dict.pkl', 'wb')\n",
    "pk.dump(features, f)\n",
    "f.close()'''\n",
    "\n",
    "#to open file :\n",
    "o = open('feature_dict.pkl', 'rb')\n",
    "features = pk.load(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = get_captions()\n",
    "clean_text(captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAKING INPUT DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separating train, dev and test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(file):\n",
    "    \n",
    "    text = []\n",
    "    with open(file) as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    names = []\n",
    "    text = text.split('\\n')\n",
    "    for image in text:\n",
    "        img_id = image.split('.')[0]\n",
    "        names.append(img_id)\n",
    "    \n",
    "    return names\n",
    "        \n",
    "\n",
    "train_id = get_dataset(os.path.join(basepath, 'Flicker8k_Text\\Flickr_8k.trainImages.txt')\n",
    "dev_id = get_dataset(os.path.join(basepath, 'Flicker8k_Text\\Flickr_8k.devImages.txt')\n",
    "test_id = get_dataset(os.path.join(basepath, 'Flicker8k_Text\\Flickr_8k.testImages.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = {}\n",
    "features_dev = {}\n",
    "features_test = {}\n",
    "\n",
    "for key in features.keys():\n",
    "    val = features[key]\n",
    "    if(key in train_id):\n",
    "        features_train[key] = val\n",
    "    elif(key in dev_id):\n",
    "        features_dev[key] = val\n",
    "    else:\n",
    "        features_test[key] = val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making Vocabulary using Tokenizer to convert the sequences into integers (Method 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(captions):\n",
    "    \n",
    "    all_captions = []\n",
    "    for img_id in captions.keys():\n",
    "        [all_captions.append(caption) for caption in captions[img_id]]\n",
    "    \n",
    "    Tx = max(len(d.split()) for d in all_captions)       #length of longest caption(caption with maximum number of words)\n",
    "            \n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(all_captions)\n",
    "    return Tx, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx, tokenizer = get_tokenizer(captions)\n",
    "vocab_size = len(tokenizer.word_index) + 1  #plus 1 for including the padding sequence (shown later in embedding layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making Input Data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The model will be provided one word and the photo and generate the next word. Then the first two words of the description will be provided to the model as input with the image to generate the next word. This is how the model will be trained.\n",
    "\n",
    "For example, the input sequence “little girl running in field” would be split into 6 input-output pairs to train the model:\n",
    "\n",
    "X1,\t\tX2 (text sequence), \t\t\t\t\t\ty (word)\n",
    "photo\tstartseq, \t\t\t\t\t\t\t\t\tlittle\n",
    "photo\tstartseq, little,\t\t\t\t\t\t\tgirl\n",
    "photo\tstartseq, little, girl, \t\t\t\t\trunning\n",
    "photo\tstartseq, little, girl, running, \t\t\tin\n",
    "photo\tstartseq, little, girl, running, in, \t\tfield\n",
    "photo\tstartseq, little, girl, running, in, field, endseq\n",
    "\n",
    "Later, when the model is used to generate descriptions, the generated words will be concatenated and recursively provided as input to generate a caption for an image.\n",
    "\n",
    "The output data will therefore be a one-hot encoded version of each word, representing an idealized probability distribution with 0 values at all word positions except the actual word position, which has a value of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(features, captions, Tx, vocab_size, tokenizer):\n",
    "    \n",
    "    X_img = []\n",
    "    X_text = []\n",
    "    Y = []\n",
    "    \n",
    "    for img_id in features.keys():\n",
    "        for caption in captions[img_id]:\n",
    "            indices = tokenizer.texts_to_sequences([caption])\n",
    "            for i in range(1, len(caption.split())):\n",
    "                inp = indices[0][:i]\n",
    "                op = indices[0][i]\n",
    "                inp = pad_sequences([inp], maxlen = Tx)[0] #output shape is (1,Tx), we only need the array of values not 2d array\n",
    "                op = to_categorical([op], num_classes = vocab_size)[0] #output shape is (1, vocab_size)\n",
    "                img_code = features[img_id][0]  #each feature[img_id] is a 2d np array of size (1, 4096). We don't need 2D array, we only want the 4096 features.\n",
    "                X_text.append(inp)\n",
    "                Y.append(op)\n",
    "                X_img.append(img_code)\n",
    "                \n",
    "    return np.asarray(X_img), np.asarray(X_text), np.asarray(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_img_train, X_text_train, Y_train = make_data(features_train, captions, Tx, vocab_size, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_img_dev, X_text_dev, Y_dev = make_data(features_dev, captions, Tx, vocab_size, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAKING THE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"merged_model.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(Tx, vocab_size):\n",
    "    \n",
    "    #Image Model\n",
    "    X_img = Input((4096,))\n",
    "    X1 = Dropout(0.5)(X_img)\n",
    "    X1 = Dense(256, activation = 'relu')(X1)\n",
    "    \n",
    "    #Text Model\n",
    "    X_text = Input((Tx, ))\n",
    "    X2 = Embedding(vocab_size, 256, mask_zero = True)(X_text) #mask_zero : for padding sequence\n",
    "    X2 = Dropout(0.5)(X2)\n",
    "    X2 = LSTM(256)(X2)\n",
    "    \n",
    "    #Decoder(Merge) Model\n",
    "    X = Add()([X1, X2])\n",
    "    X = Dense(256, activation = 'relu')(X)\n",
    "    X = Dense(vocab_size, activation = 'softmax')(X)\n",
    "    \n",
    "    model = Model(inputs = [X_img, X_text], outputs = X)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(Tx, vocab_size)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([X_img_train, X_text_train], Y_train, epochs = 20, batch_size = 1024, validation_data = ([X_img_dev, X_text_dev], Y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENERATING NEW CAPTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = VGG16()\n",
    "model1.layers.pop()  \n",
    "model1 = Model(inputs = model1.inputs, outputs = model1.layers[-1].output)\n",
    "\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "def generate_sequence(model, tokenizer, photo, Tx):\n",
    "    \n",
    "    inp = 'startseq'\n",
    "    for i in range(Tx):\n",
    "        sequence = tokenizer.texts_to_sequences([inp])\n",
    "        sequence = pad_sequences(sequence, maxlen = Tx)\n",
    "        yhat = model.predict([photo,sequence])\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        if word is None or word == 'endseq':\n",
    "            break\n",
    "        inp += ' ' + word\n",
    "        \n",
    "    return inp\n",
    "\n",
    "def generate_caption(path):\n",
    "    \n",
    "    image = pic.load_img(path, target_size = (224, 224))\n",
    "    image = pic.img_to_array(image)\n",
    "    image = image.reshape(1, image.shape[0], image.shape[1], -1)\n",
    "    image = preprocess_input(image) \n",
    "    feature_image = model1.predict(image)\n",
    "    s = generate_sequence(model, tokenizer, feature_image, Tx)\n",
    "    return s[9:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = None           #add path of required image\n",
    "generate_caption(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
